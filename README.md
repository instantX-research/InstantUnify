<div align="center">
<h1>InstantUnify</h1>

[**Qixun Wang**](https://github.com/wangqixun) · [**Xu Bai**](https://github.com/Yue02280220) · [**Haofan Wang**](https://haofanwang.github.io/)<sup>*</sup>


InstantX Team

<sup>*</sup>corresponding authors


<a href='xxxxx'><img src='https://img.shields.io/badge/Project-Page-green'></a>
<a href='xxxxx'><img src='https://img.shields.io/badge/Technique-Report-red'></a>
[![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-blue)](xxxxxxxx)
[![GitHub](https://img.shields.io/github/stars/instantX-research/InstantUnify?style=social)](https://github.com/instantX-research/InstantUnify)


</div>

InstantUnify integrates VLM into the diffusion model in the form of a plugin, enabling image-driven capabilities while maintaining the original text-to-image generation capabilities. We unify virtual fitting rooms, ID embedding, IP embedding, full-image embedding, and controlnet control into a singular visual embedding mode for images. As the variety of data formats expands, more tasks may be integrated in the future.


<div align="center">
<img src='assets/base.png' width = 900 >
</div>

Training VLM requires significant computational power. Although the current InstantUnify checkpoint has demonstrated the effectiveness of this approach, the image quality and details are still not up to expectations. We are urgently training InstantUnify, and as the training progresses, we will periodically update the results of the latest checkpoints until the final version is determined. Stay tuned for updates.